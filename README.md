# ğŸ™ï¸ AI-Powered Media Transcription & Job Tracking Pipeline (n8n)

## ğŸ“Œ Project Overview

This project is an end-to-end asynchronous media processing pipeline built with **n8n**, **OpenAI (Whisper)**, **Supabase**, and advanced webhook routing.

It simulates a high-traffic production environment where large media files are ingested, validated, and transcribed. The system provides immediate feedback to the client while handling long-running AI processing tasks in the background.

---

## ğŸ¯ Business Problem

Processing large media files for transcription often leads to:

- Hanging API connections  
- Server timeouts  
- Lost files if a process crashes  
- No tracking visibility while AI processing runs  

Without a structured asynchronous pipeline, users are left waiting without a tracking ID while the AI processes their request.

---

## âœ… Solution

This automation workflow implements a **non-blocking asynchronous architecture**:

- **Gateway Validation** â€“ Filters inbound requests based on payload size (25MB limit)
- **Immediate Acknowledgement** â€“ Returns `202 Accepted` with a unique `job_id` before heavy processing begins
- **Persistent Logging** â€“ Stores job metadata in Supabase to track status from `pending` to `completed`
- **AI Processing** â€“ Uses OpenAI Whisper for high-accuracy speech-to-text conversion
- **Final Persistence** â€“ Updates the database record with the completed transcript
- **Error Resilience** â€“ Handles oversized files using proper `413 Payload Too Large` responses

---

## ğŸ› ï¸ Tech Stack

- **n8n** â€“ Workflow orchestration and logic branching  
- **OpenAI API (Whisper)** â€“ Neural speech-to-text processing  
- **Supabase** â€“ PostgreSQL database for job state persistence  
- **HTTP / REST** â€“ Custom webhook responses and binary data handling  
- **Postman** â€“ API stress testing and payload simulation  
- **GitHub** â€“ Version control and documentation  

---

## ğŸ”„ Workflow Architecture

The system is divided into four architectural tiers:

### 1ï¸âƒ£ The Gateway (Validation)
Authenticates and filters incoming data.

### 2ï¸âƒ£ The Intake (Logging)
Records the job and issues a receipt (`job_id`) to the user.

### 3ï¸âƒ£ The Engine (Processing)
Handles resource-heavy AI transcription.

### 4ï¸âƒ£ The Persistence (Completion)
Finalizes the data lifecycle in the database.

---

## ğŸ¤– AI Transcription Logic

The **Engine** layer uses the OpenAI Whisper model.

Because AI transcription can take seconds or minutes, the workflow implements a **parallel fork pattern**:

- The user receives a response in **< 200ms**
- AI processing continues in the background
- No connection timeouts occur

This ensures scalability and a production-ready user experience.

---

## ğŸ—„ï¸ Data Storage (Supabase)

Each transaction follows a strict schema for auditability:

- `id` â€“ Unique UUID for job tracking  
- `file_name` â€“ Original file metadata  
- `status` â€“ Transitions from `processing` to `completed`  
- `transcript` â€“ Final AI-generated text output  

---

## âš ï¸ Error Handling & API Standards

This project follows RESTful API standards:

- **413 Request Entity Too Large** â€“ Triggered if the file exceeds the 25MB Whisper limit  
- **202 Accepted** â€“ Used for asynchronous processing acknowledgment  
- **200 OK** â€“ Used for successful final responses  
- **JSON Standard** â€“ All responses include `Content-Type: application/json` headers  

---

## ğŸš€ What This Project Demonstrates

- **System Thinking** â€“ Transitioning from simple scripts to multi-tier architecture  
- **Async Processing** â€“ Handling long-running tasks without blocking the UI  
- **Data Integrity** â€“ Logging state before processing to prevent job loss  
- **Professional Documentation** â€“ Clear separation of concerns and naming conventions  

---

## ğŸ“š What I Learned

Building this automation provided hands-on experience with production-grade workflow design:

- **Advanced Webhook Management** â€“ Differentiating between automatic and response-node modes to control client-facing output  
- **Parallel Execution (Forking)** â€“ Using Merge nodes and parallel paths to run background tasks while closing HTTP requests  
- **API Status Codes** â€“ Applying correct HTTP codes (`413`, `202`, `200`) to build predictable APIs  
- **Payload Handling** â€“ Managing binary data buffers to move files from webhook to cloud AI engines efficiently  

---

## ğŸ‘¤ Author

**Adeola Olagbenro**  
Automation Engineer / Workflow Developer
